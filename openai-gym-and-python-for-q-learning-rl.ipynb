{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7086a433",
   "metadata": {
    "papermill": {
     "duration": 0.022749,
     "end_time": "2022-02-07T19:20:12.920640",
     "exception": false,
     "start_time": "2022-02-07T19:20:12.897891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning - Developing Intelligent Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb63a6",
   "metadata": {
    "papermill": {
     "duration": 0.021135,
     "end_time": "2022-02-07T19:20:12.963556",
     "exception": false,
     "start_time": "2022-02-07T19:20:12.942421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h4 style='color:CornflowerBlue;'>Deep Learning Course 5 of 6 - Level: Advanced</h4>\n",
    "<p style='color:DeepSkyBlue;'>Follow up <a href='https://deeplizard.com/'>Deeplizrard</a> for more information</p>\n",
    "\n",
    "1. <a href='https://deeplizard.com/learn/video/QK_PP_2KgGE'>OpenAI Gym and Python for Q-learning - Reinforcement Learning Code Project</a>\n",
    "2. <a href='https://deeplizard.com/learn/video/HGeI30uATws'>Train Q-learning Agent with Python - Reinforcement Learning Code Project</a>\n",
    "3. <a href='https://deeplizard.com/learn/video/ZaILVnqZFCg'>Watch Q-learning Agent Play Game with Python - Reinforcement Learning Code Project</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f326160",
   "metadata": {
    "papermill": {
     "duration": 0.021153,
     "end_time": "2022-02-07T19:20:13.006342",
     "exception": false,
     "start_time": "2022-02-07T19:20:12.985189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='5' color='CornflowerBlue'>OpenAI Gym and Python for Q-learning - Reinforcement Learning Code Project</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de59fc70",
   "metadata": {
    "papermill": {
     "duration": 0.022009,
     "end_time": "2022-02-07T19:20:13.050000",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.027991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DarkSlateBlue'><b>OpenAI Gym</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ef828",
   "metadata": {
    "papermill": {
     "duration": 0.021469,
     "end_time": "2022-02-07T19:20:13.093288",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.071819",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "we'll be using Python and OpenAI Gym to develop our reinforcement learning algorithm. The Gym library is a collection of environments that we can use with the reinforcement learning algorithms we develop.\n",
    "\n",
    "Gym has a ton of environments ranging from simple text based games to Atari games like Breakout and Space Invaders. The library is intuitive to use and simple to install. Just run pip install gym, and you're good to go! [The link to Gym's installation instructions, requirements, and documentation](https://gym.openai.com/docs/) is included in the description. Go ahead and get that installed now because we'll need it in just a moment. \n",
    "\n",
    "<center>\n",
    "<img src='https://deeplizard.com/assets/svg/ac9a374b.svg'/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cd9467",
   "metadata": {
    "papermill": {
     "duration": 0.021274,
     "end_time": "2022-02-07T19:20:13.136722",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.115448",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " We'll be making use of Gym to provide us with an environment for a simple game called Frozen Lake. We'll then train an agent to play the game using Q-learning, and we'll get a playback of how the agent does after being trained.\n",
    "\n",
    "So, let's jump into the details for **Frozen Lake**! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf697f",
   "metadata": {
    "papermill": {
     "duration": 0.021136,
     "end_time": "2022-02-07T19:20:13.179237",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.158101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DeepSkyBlue'><b>Overview</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad80df",
   "metadata": {
    "papermill": {
     "duration": 0.021158,
     "end_time": "2022-02-07T19:20:13.222129",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.200971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This grid is our environment where `S` is the agent's starting point, and it's safe. `F` represents the frozen surface and is also safe. `H` represents a hole, and if our agent steps in a hole in the middle of a frozen lake, well, that's not good. Finally, `G` represents the goal, which is the space on the grid where the prized frisbee is located.\n",
    "\n",
    "The agent can navigate left, right, up, and down, and the episode ends when the agent reaches the goal or falls in a hole. It receives a reward of one if it reaches the goal, and zero otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d9cb7",
   "metadata": {
    "papermill": {
     "duration": 0.021046,
     "end_time": "2022-02-07T19:20:13.265013",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.243967",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DeepSkyBlue'><b>Table Dictionary</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552a823",
   "metadata": {
    "papermill": {
     "duration": 0.024517,
     "end_time": "2022-02-07T19:20:13.310827",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.286310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th>State</th>\n",
    "    <th>Description</th>\n",
    "    <th>Reward</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Agent's starting point - safe&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Frozen surface - safe&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;H&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hole - game over&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Goal - game over&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "    <td>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afaa1b9",
   "metadata": {
    "papermill": {
     "duration": 0.020889,
     "end_time": "2022-02-07T19:20:13.353017",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.332128",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33370f",
   "metadata": {
    "papermill": {
     "duration": 0.021015,
     "end_time": "2022-02-07T19:20:13.395413",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.374398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DeepSkyBlue'><b>Configurations</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79de67",
   "metadata": {
    "papermill": {
     "duration": 0.020873,
     "end_time": "2022-02-07T19:20:13.437474",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.416601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Install **Gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2efdfb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:13.486843Z",
     "iopub.status.busy": "2022-02-07T19:20:13.485061Z",
     "iopub.status.idle": "2022-02-07T19:20:24.364934Z",
     "shell.execute_reply": "2022-02-07T19:20:24.365411Z"
    },
    "papermill": {
     "duration": 10.906974,
     "end_time": "2022-02-07T19:20:24.365725",
     "exception": false,
     "start_time": "2022-02-07T19:20:13.458751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e100030",
   "metadata": {
    "papermill": {
     "duration": 0.022058,
     "end_time": "2022-02-07T19:20:24.412249",
     "exception": false,
     "start_time": "2022-02-07T19:20:24.390191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Apply using auto-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df2d0e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:24.472494Z",
     "iopub.status.busy": "2022-02-07T19:20:24.467070Z",
     "iopub.status.idle": "2022-02-07T19:20:24.478043Z",
     "shell.execute_reply": "2022-02-07T19:20:24.478593Z"
    },
    "papermill": {
     "duration": 0.044322,
     "end_time": "2022-02-07T19:20:24.478822",
     "exception": false,
     "start_time": "2022-02-07T19:20:24.434500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1462d3b7",
   "metadata": {
    "papermill": {
     "duration": 0.02409,
     "end_time": "2022-02-07T19:20:24.526958",
     "exception": false,
     "start_time": "2022-02-07T19:20:24.502868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DeepSkyBlue'><b>Libraries</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83f9d3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:24.583223Z",
     "iopub.status.busy": "2022-02-07T19:20:24.582088Z",
     "iopub.status.idle": "2022-02-07T19:20:25.054054Z",
     "shell.execute_reply": "2022-02-07T19:20:25.052903Z"
    },
    "papermill": {
     "duration": 0.502168,
     "end_time": "2022-02-07T19:20:25.054243",
     "exception": false,
     "start_time": "2022-02-07T19:20:24.552075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e078d",
   "metadata": {
    "papermill": {
     "duration": 0.022692,
     "end_time": "2022-02-07T19:20:25.100298",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.077606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf31b2e",
   "metadata": {
    "papermill": {
     "duration": 0.024171,
     "end_time": "2022-02-07T19:20:25.147926",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.123755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DeepSkyBlue'><b>Create the Environment</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ab5942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:25.202708Z",
     "iopub.status.busy": "2022-02-07T19:20:25.201578Z",
     "iopub.status.idle": "2022-02-07T19:20:25.217873Z",
     "shell.execute_reply": "2022-02-07T19:20:25.218476Z"
    },
    "papermill": {
     "duration": 0.045487,
     "end_time": "2022-02-07T19:20:25.218707",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.173220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe42cc1",
   "metadata": {
    "papermill": {
     "duration": 0.023989,
     "end_time": "2022-02-07T19:20:25.265937",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.241948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DeepSkyBlue'><b>Create the Q-table</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97f4439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:25.321434Z",
     "iopub.status.busy": "2022-02-07T19:20:25.320665Z",
     "iopub.status.idle": "2022-02-07T19:20:25.322062Z",
     "shell.execute_reply": "2022-02-07T19:20:25.322702Z"
    },
    "papermill": {
     "duration": 0.032588,
     "end_time": "2022-02-07T19:20:25.322906",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.290318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1438678c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:25.374410Z",
     "iopub.status.busy": "2022-02-07T19:20:25.373382Z",
     "iopub.status.idle": "2022-02-07T19:20:25.382233Z",
     "shell.execute_reply": "2022-02-07T19:20:25.382868Z"
    },
    "papermill": {
     "duration": 0.036756,
     "end_time": "2022-02-07T19:20:25.383085",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.346329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee812e",
   "metadata": {
    "papermill": {
     "duration": 0.024004,
     "end_time": "2022-02-07T19:20:25.431104",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.407100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DarkTurquoise'><b>Initializing Q-learning parameters</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681b6a66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:25.483207Z",
     "iopub.status.busy": "2022-02-07T19:20:25.482420Z",
     "iopub.status.idle": "2022-02-07T19:20:25.488721Z",
     "shell.execute_reply": "2022-02-07T19:20:25.488099Z"
    },
    "papermill": {
     "duration": 0.033613,
     "end_time": "2022-02-07T19:20:25.488911",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.455298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_episodes = int(1e4)\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "lr = 1e-1\n",
    "dr = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exp_rate = 1\n",
    "min_exp_rate = 1e-3\n",
    "exp_decay_rate = 1e-3\n",
    "\n",
    "# building a reward list for all the episodes\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646b797",
   "metadata": {
    "papermill": {
     "duration": 0.02532,
     "end_time": "2022-02-07T19:20:25.540167",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.514847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DarkTurquoise'><b>Building the Q-learning Algorithm</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a838eb8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:25.591802Z",
     "iopub.status.busy": "2022-02-07T19:20:25.591085Z",
     "iopub.status.idle": "2022-02-07T19:20:36.158120Z",
     "shell.execute_reply": "2022-02-07T19:20:36.157453Z"
    },
    "papermill": {
     "duration": 10.594608,
     "end_time": "2022-02-07T19:20:36.158300",
     "exception": false,
     "start_time": "2022-02-07T19:20:25.563692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    # reset the movements in the env\n",
    "    state = env.reset()\n",
    "    # check if the agent reaches the target\n",
    "    done = False\n",
    "    \n",
    "    # variable for expected return G_t\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    # for loop for each step for the agent\n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # apply epsilon greedy stategy\n",
    "        random_number = random.uniform(0, 1)\n",
    "        # Exploration Vs. Exploitation trade-off\n",
    "        if random_number > exploration_rate:\n",
    "            # start exploitation ---> getting the maximum Q-value from the possible movements of his current state.\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            # start exploration ---> select any random action to explore a random state.\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # after taking the action, we're going to update our agent with the new info, rewards, state, and if he reaches the end or not!\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update our Q-table for Q(s, a) using Bellman Equation\n",
    "                                            # Old Q-value\n",
    "        q_table[state, action] = (1 - lr) * q_table[state, action] + \\\n",
    "                                 lr * (reward + dr*(np.max(q_table[new_state, :])))\n",
    "                                            # learned value\n",
    "\n",
    "        # transition to the next state\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        # check to see if our last action ended the episode for us,\n",
    "        # meaning, did our agent step in a hole or reach the goal?\n",
    "        if done:\n",
    "            break\n",
    "        # If the action did end the episode, then we jump out of this loop and move on to the next episode.\n",
    "        # Otherwise, we transition to the next time-step.\n",
    "    \n",
    "    # Exploration Rate Decay\n",
    "    # https://en.wikipedia.org/wiki/Exponential_decay\n",
    "    exploration_rate = min_exp_rate + \\\n",
    "                      (max_exp_rate - min_exp_rate) * np.exp(-exp_decay_rate * episode)\n",
    "    \n",
    "    # append the current rewards in the list of rewards\n",
    "    rewards_all_episodes.append(rewards_current_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc0015",
   "metadata": {
    "papermill": {
     "duration": 0.023843,
     "end_time": "2022-02-07T19:20:36.207214",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.183371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DarkTurquoise'><b>Examinating the rewards</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd47c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:36.264596Z",
     "iopub.status.busy": "2022-02-07T19:20:36.263588Z",
     "iopub.status.idle": "2022-02-07T19:20:36.270240Z",
     "shell.execute_reply": "2022-02-07T19:20:36.270797Z"
    },
    "papermill": {
     "duration": 0.03939,
     "end_time": "2022-02-07T19:20:36.271022",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.231632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************Average rewards per thousand episodes********************************\n",
      "Count No. 1,000: 0.036000000000000025\n",
      "Count No. 2,000: 0.23200000000000018\n",
      "Count No. 3,000: 0.4080000000000003\n",
      "Count No. 4,000: 0.6100000000000004\n",
      "Count No. 5,000: 0.6690000000000005\n",
      "Count No. 6,000: 0.6700000000000005\n",
      "Count No. 7,000: 0.7370000000000005\n",
      "Count No. 8,000: 0.7020000000000005\n",
      "Count No. 9,000: 0.6860000000000005\n",
      "Count No. 10,000: 0.7130000000000005\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average rewards per thousand episodes\".center(100, '*'))\n",
    "for reward in rewards_per_thousand_episodes:\n",
    "    print(f'Count No. {count:,}: {sum(reward/1000)}')\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d394426",
   "metadata": {
    "papermill": {
     "duration": 0.02426,
     "end_time": "2022-02-07T19:20:36.320410",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.296150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the printout we can notice that our average reward per thousand epoisodes did indeed progress overtime. When the algorithm first start training, the first thousands episodes only average a reward of `0.062`, but by the time it got to its last thousand episodes, the reward improved to `0.746`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaee743",
   "metadata": {
    "papermill": {
     "duration": 0.024206,
     "end_time": "2022-02-07T19:20:36.369457",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.345251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DarkTurquoise'><b><a href='https://deeplizard.com/learn/video/HGeI30uATws#:~:text=%20interpreting%20the%20training%20results%20'>Interpreting the training results</a></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a84018",
   "metadata": {
    "papermill": {
     "duration": 0.024187,
     "end_time": "2022-02-07T19:20:36.418342",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.394155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our agent played `10,000` episodes. At each time step within an episode, the agent received a reward of `1` if it reached the frisbee, otherwise, it received a reward of `0`. If the agent did indeed reach the frisbee, then the episode finished at that time-step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08168d",
   "metadata": {
    "papermill": {
     "duration": 0.024257,
     "end_time": "2022-02-07T19:20:36.467225",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.442968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, that means for each episode, the total reward received by the agent for the entire episode is either `1` or `0`. So, for the first thousand episodes, we can interpret this score as meaning that  **6%** of the time, the agent received a reward of `1` and won the episode. And by the last thousand episodes from a total of 10,000, the agent was winning **74%** of the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d91919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:36.522400Z",
     "iopub.status.busy": "2022-02-07T19:20:36.521708Z",
     "iopub.status.idle": "2022-02-07T19:20:36.532785Z",
     "shell.execute_reply": "2022-02-07T19:20:36.533457Z"
    },
    "papermill": {
     "duration": 0.040316,
     "end_time": "2022-02-07T19:20:36.533717",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.493401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************Q-Table***********************************************\n",
      "\n",
      "                          [0.53069746 0.46588523 0.4673052  0.48438888]\n",
      "                          [0.34167083 0.29042392 0.25846081 0.48588965]\n",
      "                          [0.45182109 0.26290912 0.26920019 0.26475097]\n",
      "                          [0.12672784 0.01115926 0.08393566 0.18697262]\n",
      "                          [0.54567205 0.42629454 0.35622466 0.22699947]\n",
      "                          [0. 0. 0. 0.]\n",
      "                          [0.13525455 0.12149018 0.31688375 0.09611634]\n",
      "                          [0. 0. 0. 0.]\n",
      "                          [0.25687289 0.4838103  0.35185843 0.58401692]\n",
      "                          [0.32303812 0.63752144 0.52363538 0.37934846]\n",
      "                          [0.62699245 0.38056755 0.3594257  0.3435708 ]\n",
      "                          [0. 0. 0. 0.]\n",
      "                          [0. 0. 0. 0.]\n",
      "                          [0.47162332 0.4131296  0.70100964 0.45809895]\n",
      "                          [0.71407938 0.91152129 0.73412863 0.71488013]\n",
      "                          [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-Table\".center(100, '*'))\n",
    "print()\n",
    "\n",
    "for row in q_table:\n",
    "    print(' '* 25, row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add183c0",
   "metadata": {
    "papermill": {
     "duration": 0.024806,
     "end_time": "2022-02-07T19:20:36.584014",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.559208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size='4' color='DarkTurquoise'><b>Building the Q-learning Interface</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832c2a2f",
   "metadata": {
    "papermill": {
     "duration": 0.024756,
     "end_time": "2022-02-07T19:20:36.635186",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.610430",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's see how interactively the agent plays **Frozen Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3203a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-07T19:20:36.691336Z",
     "iopub.status.busy": "2022-02-07T19:20:36.690572Z",
     "iopub.status.idle": "2022-02-07T19:22:52.859355Z",
     "shell.execute_reply": "2022-02-07T19:22:52.858704Z"
    },
    "papermill": {
     "duration": 136.198359,
     "end_time": "2022-02-07T19:22:52.859528",
     "exception": false,
     "start_time": "2022-02-07T19:20:36.661169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "***************You reach the goal!****************\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(f'Episode: {episode+1}'.center(50, '='))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # for clearning the board\n",
    "        clear_output(wait=True)\n",
    "        # allows you to check the agent's environment\n",
    "        env.render()\n",
    "        time.sleep(0.4)\n",
    "        \n",
    "        # invoke the action with the highest Q-value from the Q-Table for the current state\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        # take the action and move to the new state\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # acting condition\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print('You reach the goal!'.center(50, '*'))\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('You fall through a hole!'.center(50, '-'))\n",
    "                time.sleep(3)\n",
    "                clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "        # select the new state based on the agent action\n",
    "        state = new_state\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3433a8",
   "metadata": {
    "papermill": {
     "duration": 0.025599,
     "end_time": "2022-02-07T19:22:52.910776",
     "exception": false,
     "start_time": "2022-02-07T19:22:52.885177",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "***Applied by [Ahmed](https://www.linkedin.com/in/ai-ahmed/) â€“ Environment [Gradient](https://console.paperspace.com/ai-ahmed/notebook/r1v841exffrzbek)***\n",
    "- Github: [AI-Ahmed](https://github.com/AI-Ahmed)\n",
    "- Kaggle: [Ahmed](https://www.kaggle.com/dsxavier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 171.459746,
   "end_time": "2022-02-07T19:22:53.951509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-07T19:20:02.491763",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
